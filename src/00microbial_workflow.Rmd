---
title: "MP Data: DADA2 and R Statistics Processing"
output:
  html_document: default
  pdf_document: default
editor_options:
  chunk_output_type: console
---

This document contains DADA2 CODE from [1.8 and 1.16 Tutorials](https://benjjneb.github.io/dada2/tutorial.html).

If you are visualizing this document inside RStudio, try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. But clicking the *Run* button on the upper right hand corner of each chunk will run the code. Also note that `{r, eval=FALSE, include=TRUE}` is an easier way to save the RMarkdown file and output it to HTML/PDF without running all of the code (since that would take forever) and showing the output. 


## Introduction

The starting point of the DADA2 pipeline is a set of Illumina-sequenced paired-end fastq files that have been split by sample (demultiplexed) and from which barcodes/adapters have been removed. The end product of DADA2 is an amplicon sequence variant (ASV - Callahan et al. 2017) table, a higher-resolution analogue of OTU tables, which records the number of times each exact amplicon sequence variant was observed in each sample. Taxonomy is also assigned to the output sequences, and the pipeline demonstrates how the data can be imported into the popular phyloseq R package (and other packages) for the analysis of 16S rDNA microbiome data. R statistical packages will be utilized to analyze and visualize data.

First time around, you can check the version of R and R Studio you have. Look under `version.string` after running this command:

```{r, eval=FALSE, include=TRUE}
version

# 27 DEC 2021: version.string is 4.0.4 (2021-02-15)
```

*Things to note:* when you are working in R and running the code below, your "objects" will be created under the "Environment" tab. Make sure you save the environment (.RData) every time you exit R, and open the environment when you start back up. That way, you won't have to run all the code over again since your "objects" will be saved. That being said, keep track of the changes you've made to each object, since they will be referenced in different parts of this protocol. Also remember, you will need to re-load all packages when you restart R.

## Install DADA2 (first-time users)

It's time to download/install DADA2. Make sure you type "a" and "y" on the console when appropriate, during download. YOU ONLY NEED TO DO THIS ONCE.

```{r, eval=FALSE, include=TRUE}
install.packages("BiocManager")
BiocManager::install("benjjneb/dada2", force = TRUE)
packageVersion("dada2")

```

## Install packages (first-time users)

You also need to install some important packages to process data, visualize it, run statistics, and create publishable figures. YOU ONLY NEED TO DO THIS ONCE. Run the first line of code to install BiocManager only if you didn't run it above when installing dada2.

```{r, eval=FALSE, include=TRUE}
# install.packages("BiocManager")
BiocManager::install("phyloseq")
BiocManager::install("ggplot2", force = TRUE)
BiocManager::install("vegan", force = TRUE)
BiocManager::install("DESeq2", force = TRUE)
BiocManager::install("tidyverse", force = TRUE)
BiocManager::install("ggpubr", force = TRUE)
BiocManager::install("microbiome", force = TRUE)
install.packages("FSA")
install.packages("PMCMRplus")
install.packages("moments")
install.packages("nortest")
install.packages("grid")
install.packages("gridExtra")
install.packages("RVAideMemoire")
install.packages("magrittr")
install.packages("dplyr")
install.packages("scales")
install.packages("reshape2")
install.packages("ape")
install.packages("lme4")
install.packages("plotly")
install.packages("tidyr")
install.packages("randomcoloR")
install.packages("remotes")
remotes::install_github("MadsAlbertsen/ampvis2", Ncpus = 6, force = TRUE)
devtools::install_github("madsalbertsen/ampvis2", host = "https://api.github.com")
```


## Load packages

Every time you restart the RStudio app program, you will need to load the packages needed into the system library so that you can use its functions. 

```{r, eval=FALSE, include=TRUE}
library("dada2"); package.version("dada2") # package version 1.18.0 on 28 DEC 2021
library("phyloseq")
library("ggplot2")
library("vegan")
library("DESeq2")
library("tidyverse")
library("ggpubr")
library("microbiome")
library("FSA")
library("PMCMRplus")
library("moments")
library("nortest")
library("grid")
library("gridExtra")
library("RVAideMemoire")
library("magrittr")
library("dplyr")
library("scales")
library("reshape2")
library("ape")
library("lme4")
library("plotly")
library("tidyr")
library("randomcoloR")
library("remotes")
library("ampvis2")
```

## Set working directory

Set the working directory (the folder you will be working out of) which contains the unzipped fastq files of your samples. Then you can visualize via a list which files are in your working directory. The list should be the names of your sequence sample files (fastq or fastq.gz)

```{r, eval=FALSE, include=TRUE}
setwd("/Volumes/MaiteBackupHD/MPData/MP_515_806")
path <- "/Volumes/MaiteBackupHD/MPData/MP_515_806"
list.files(path)
```

Prior to starting, we'll open all .gz files in the directory to turn them into .fastq files, placing them in a new folder within our directory.

Additionally, we'll select all files, right click, and rename so that no "-" are present in the sample name. Make sure to make this change in the metadata file including all sequence file sample data. 

## Pre-processing and quality filtering

Read in the names of the fastq files and perform string manipulation to get matched lists of the forward (F) and reverse (R) files. For example, F and R fastq file names may have format: SAMPLENAME_S###_L00#_R1_001.fastq and SAMPLENAME_S###_L00#_R1_002.fastq. Check your file names under the "Set working directory" section of this document.

```{r, eval=FALSE, include=TRUE}
fnFs <- sort(list.files(path, pattern="_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001.fastq", full.names = TRUE))
```

Extract sample names (format should be "SAMPLENAME_S##_L001_R#_001.fastq"). Then confirm your sequence file sample names (these will be used to identify samples and their metadata in a future metadata file). Note that the number on the left will indicate how many samples you have.

```{r, eval=FALSE, include=TRUE}
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
sample.names # 151 sample/file names
```

Inspect read quality profile for F and R reads. You can change `1:10` to say `1:5` or even `3:18` based on which samples you want to look at.

```{r, eval=FALSE, include=TRUE}
plotQualityProfile(fnFs[1:12])
plotQualityProfile(fnFs[13:24])
plotQualityProfile(fnFs[25:36])
plotQualityProfile(fnFs[37:48])
plotQualityProfile(fnFs[49:60])
plotQualityProfile(fnFs[61:72])
plotQualityProfile(fnFs[73:84])
plotQualityProfile(fnFs[85:96])
plotQualityProfile(fnFs[96:108])
plotQualityProfile(fnFs[109:120])
plotQualityProfile(fnFs[121:132])
plotQualityProfile(fnFs[133:144])
plotQualityProfile(fnFs[145:151])
# The above quality profiles were saved to directory on 28 DEC 2021

plotQualityProfile(fnRs[1:12])
plotQualityProfile(fnRs[13:24])
plotQualityProfile(fnRs[25:36])
plotQualityProfile(fnRs[37:48])
plotQualityProfile(fnRs[49:60])
plotQualityProfile(fnRs[61:72])
plotQualityProfile(fnRs[73:84])
plotQualityProfile(fnRs[85:96])
plotQualityProfile(fnRs[96:108])
plotQualityProfile(fnRs[109:120])
plotQualityProfile(fnRs[121:132])
plotQualityProfile(fnRs[133:144])
plotQualityProfile(fnRs[145:151])
```

*Plot description:* Gray scale: heat map of the frequency of each quality score (y axis) at each base position (x axis), the median QS at each position is shown by the green line, and quartiles by the orange line. The red line shows scaled proportion of reads that extend to at least that position (will be a flat line since for 16S data, the reads are the sample length).

It is typical that R reads are worse quality, and DADA2 will incorporate quality information into its error model. You will need to choose a position that reflects where the quality distribution crashes (where the green lines and dotted orange lines decrease dramatically). In the tutorial, it is around 160. For these data, it is around 160 as well. If you're using V1-V2 or V3-V4 regions for your data, you'll need to make sure your F and R reads still overlap after truncating/trimming.

Since the last few nucleotides are less well-controlled errors, you will trim them to position 240. But first you will assign filenames for the filtered fastq.gz files (aka the files names for the files that will undergo quality control/trimming/filtering). In the next step, you'll place filtered files in a "filtered" subdirectory/folder. 

```{r, eval=FALSE, include=TRUE}
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq"))

names(filtFs) <- sample.names
names(filtRs) <- sample.names
```

Now you can trim/filter the samples. The standard filtering parameters are `truncLen=c(240,160)`, `maxN=0`, `maxEE=c(2,2)`, and `truncQ=2`. On Windows set `multithread=FALSE`. 

If with default parameters, you lose a lot of reads (i.e. 28-40%), you can increase the `maxEE` (or maximum number of expected errors) for reverse reads. You can tighten or relax `maxEE`. If you want to speed up computation, decrease the second number in `maxEE` or if too few of reads are passing the filter, increase `maxEE` (i.e. to `maxEE=c(2,5)`). You can also reduce `truncLen` to remove more of the low quality tails.

You can determine what percent of reads were lost using the step following this one.

```{r, eval=FALSE, include=TRUE}
out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(240,160),
                     maxN=0, 
                     maxEE=c(2,2), 
                     truncQ=2, 
                     rm.phix=TRUE,
                     compress=TRUE, 
                     multithread=TRUE) # start 15:17 end 15:25, start 10:18 end 10:28
head(out)
out.data.frame <- as.data.frame(out)
```

You should be able to see a "filtered" folder in Finder under your working directory. 

Take a second to deviate from the DADA2 tutorial and look at what % of reads for each sample was removed. This will help determine if the parameters in the previous step were too strict (i.e. too much data loss in the % of reads processed). 

```{r, eval=FALSE, include=TRUE}
out.data.frame$percent.removed <- (1 - (out.data.frame$reads.out/out.data.frame$reads.in)) * 100
head(out.data.frame)
write.csv(out.data.frame, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Tracking Reads/reads_post_filter_and_trim.csv")

# 28 DEC 2021: the percent of reads lost range from 3% to 17%. This means our trimming/filtering is efficient and did not cause major data loss.

# 08 FEB 2022: same comment as above.
```

In mothur after trim.seqs, we removed 27% of total reads and according to the protocol 22% was a lot. For DADA2, by increasing the R read `maxEE` we removed about 30% of reads in each sample. This is probably due to the stricter filtering parameter of truncLen (in mothur we did 200 to 275). 

## Error rates 

DADA2 uses a parametric error model (`err`) and the `learnErrors` function uses the data to calculate error rates by alternating estimation of the error rate and inference of sample composition until they converge (it begins with an initial guess).

Calculate error rate for F and R reads.Then visualize them.

```{r, eval=FALSE, include=TRUE}
errF <- learnErrors(filtFs, multithread=TRUE) # start 15:29 end 15:35, start 10:33 end 10:40
# 28 DEC 2021: 107832960 total bases in 449304 reads from 9 samples used for learning error rates. 
# 08 FEB 2022: same as above.

errR <- learnErrors(filtRs, multithread=TRUE) # start 15:35 end 15:44, start 10:40 end 10:50
# 28 DEC 2021: 100944160 total bases in 630901 reads from 12 samples used for learning error rates. 
# 08 FEB 2022: same as above.

plotErrors(errF, nominalQ=TRUE)
# 28 DEC 2021: plots look satisfactory.

plotErrors(errR, nominalQ=TRUE)
# 28 DEC 2021: plots look worse than the forward reads, but that's expected.

```

The different graphs represent possible nucleotide transitions (i.e. A2C means possibility that what was supposed to be an A is a C). Note: A2A, C2C, G2G and T2T will be different since those transitions are not really errors. The points are observed error rates for each consensus quality score, black line estimating error rates, red line error rates under nominal definition of Q score. *Look for:* estimated error rates (black line) being a good fit to the observed error rates (points) and the error rates drop with increased quality (x axis). If the model doesn't look like a good fit - you can increase the `nbases` parameter within `learnErrors` (increasing the # of bases used to make the model).

## Sample Inference

This section uses the core sample inference algorithm on filtered/trimmed data to identify how many reads and unique sequences are present in each sample. Pooling the information across samples can increase sensitivity to sequence variants (ASVs) that may be present at very low frequencies in multiple samples. So you can choose to apply standard pooled processing where all samples are pooled together for sample inference. There is the option to pseudo-pool where samples are processed independently after sharing information between samples, approximating pooled sample inference in linear time. 

For this project, you could choose the pooling option to resolve conditionally rare taxa (CRTs). But see notes in code.

```{r, eval=FALSE, include=TRUE}

dadaFs <- dada(filtFs, err=errF, multithread=TRUE, pool="pseudo") 

# 28 DEC 2021: pooling the samples didn't work, start 15:52 end 16:06 with error: vector memory exhausted (limit reached?)
# 28 DEC 2021:running without pooling; start 16:06 end 16:23 but all dada-class parameters (i.e. dadaFs$map) were NULL which messed up the next step
# 28 DEC 2021:trying with pseudo-pool; start 9:11 end 10:08; pseudo-pooling (independent sample processing) has two major advantages: (1) computation time is linear in the number of samples and memory requirements are flat with the number of samples. (2) Pooling allows info to be shared across samples which makes it easier to resolve rare variants (i.e. potentially conditionally rare taxa) that were just seen once or twice in one sample by many times across samples; for more info visit [here](http://benjjneb.github.io/dada2/pseudo.html#Pseudo-pooling).

# 08 FEB 2021: start 10:51 end 11:57

dadaRs <- dada(filtRs, err=errR, multithread=TRUE, pool="pseudo") # start 10:08 end 11:11, start 11:57 end NA

```


## Merge paired ends

You are ready to merge paired reads (forward and reverse reads) by aligning the denoised F reads with R complements and construct merged contig sequence. This will only work if they overlap by at least 12 bases and are identical to each other in the overlap region.

```{r, eval=FALSE, include=TRUE}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE) # start 13:00 end before 13:18
```

You can look in your console output to see if everything successfully merged. Let's inspect the merger data.frame from the first sample. The mergers object is a list of the data.frames (aka sequences) from each sample and it holds the merged `$sequence` and `$abundance` along with `$forward` and `$reverse` sequence variants that were merged.

```{r, eval=FALSE, include=TRUE}
head(mergers[[1]])
```

## ASV table

Time to construct a sequence table (would be the OTU table in other pipelines). This table is made up of *amplicon sequence variants (ASVs)* which is a higher resolution of OTU tables.

```{r, eval=FALSE, include=TRUE}
seqtab <- makeSequenceTable(mergers)
```

The second number given by the following code will tell you the # of ASVs. Then you can inspect distribution of sequence lengths (there will be some 1s and other #s).

```{r, eval=FALSE, include=TRUE}
dim(seqtab) # 29 DEC 2021: out of 151 samples, there are 55405 ASVs.
# 08 FEB 2022: same as above.

table(nchar(getSequences(seqtab))) # 29 DEC 2021: this table ranges from 240 to 383 (range for V4 amplicon region) and the bottom numbers equal the number of samples with that many basepairs (?)
```

## Chimera removal

Remove chimeras that are identified if they can be exactly reconstructed by combining a left-segment and right-segment from two more abundant "parents".

```{r, eval=FALSE, include=TRUE}
seqtab.nochim <- removeBimeraDenovo(seqtab, 
                                    method="consensus", 
                                    multithread=TRUE, 
                                    verbose=TRUE) # start 11:29 end 11:48, start 13:24 end 13:41

# 29 DEC 2021: Identified 21197 bimeras out of 55405 input sequences.
# 08 FEB 2022: same as above.

seqtab.nochim.colnames <- colnames(seqtab.nochim)
# This is used for a future step.
```

Look at the second number in the output for the code below to see how many ASVs are left (used seqtab column names). The following line will tell you how much of your data was NOT chimeric.

```{r, eval=FALSE, include=TRUE}
dim(seqtab.nochim)
# 30 DEC 2021: the dimensions of seqtab.nochim can be compared to those of seqtab to determine how many ASVs are left after chimera removal. You see here that for 151 samples, there are 34208 ASVs.
# 08 FEB 2022: same as above.

sum(seqtab.nochim)/sum(seqtab)
#  DEC 2021: 17.84% (1-0.8216) of our data, when accounting for abundances of ASVs (variants), were chimeras. 
# 08 FEB 2022: same as above.
```

Looks like 82.16% of reads stayed after chimera removal. Now, track the reads through each step in the pipeline, and look for the majority of the reads to have made it through each step.

```{r, eval=FALSE, include=TRUE}
getN <- function(x) sum(getUniques(x))

track <- cbind(out, 
               sapply(dadaFs, getN), 
               sapply(dadaRs, getN), 
               sapply(mergers, getN), 
               rowSums(seqtab.nochim),
               rowSums(seqtab.nochim)/sapply(dadaFs, getN)) 
# added a step to track percentage of reads kept from filtered reads 

colnames(track) <- c("input", 
                     "filtered", 
                     "denoisedF", 
                     "denoisedR", 
                     "merged", 
                     "nonchim",
                     "percent_nonchim_to_filtered")

rownames(track) <- sample.names
head(track)
write.csv(track, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Tracking Reads/reads_post_chim_removal.csv")
```

## Export ASV table

In case you want to see your ASV table in excel. Note the row numbers in R in the data frame are samples and columns are ASVs (full sequence). The numbers represented are actual hits, not percentages.

```{r, eval=FALSE, include=TRUE}
asv.table <- as.data.frame(seqtab.nochim)

write.csv(asv.table, "/Volumes/MaiteBackupHD/MPData/MP_515_806/ASV Tables/asv_table_v2.csv")
# 08 FEB 2022: switched name to "..._v2.csv".
```

## Taxonomy assignment

Taxonomy is assigned via native implementation of the naive Bayesian classifier method. `assignTaxonomy` takes the input sequences and a training set of reference sequences and outputs taxonomic assignments with at least `minBoot` bootstrap confidence (default is 50 for sequences of 250nts or less), at a higher threshold (say 80), we might not get classification down to genus.

At this point, you'll need to download the trained taxonomy datasets from the tutorial website. Make sure you download them to the folder you set as your working directory above.

Run `assignTaxonomy`. The return value of this is a character matrix with each row corresponding to an input sequence and each column corresponding to a taxonomic level (1=Kingdom, 2=Phylum, 3=Class, 4=Order, 5=Family, 6=Genus).

```{r, eval=FALSE, include=TRUE}
taxa <- assignTaxonomy(seqtab.nochim,
                      "/Volumes/MaiteBackupHD/MPData/DADA2_file_MiDAS_4.8.1.fa",
                      multithread=TRUE) # start 11:19 end 11:46, start 16:29 end before 19:38

# 08 FEB 2021: changed the taxonomy database to MiDAS instead of SILVA. Includes species.

taxa.print <- taxa # removes sequence row names for display only
rownames(taxa.print) <- NULL
head(taxa.print)
# note that chloroplasts and mitochondria will be removed in a later step

taxa.data.frame <- as.data.frame(taxa) # create a .csv version
write.csv(taxa.data.frame, "/Volumes/MaiteBackupHD/MPData/MP_515_806/ASV Tables/asv_table_w_taxa_v2.csv")
```

At this point, we move into phyloseq to continue with some sequence processing followed by analysis. 

## Import to phyloseq

`seqtab.nochim` (the object we used to assign taxonomy) is a matrix with the sequences (ASVs) as column names and samples (i.e. coral-10) as row names. Here are the sample names and in what order they are in. I'm doing this instead of opening at the actual `seqtab.nonchim` data in excel because it's a very large matrix and it might freeze R.

```{r, eval=FALSE, include=TRUE}
View(seqtab.nochim)

# This part will be used in a later step.
seqtab.nonchim.rownames <- as.matrix(rownames(seqtab.nochim))
View(seqtab.nonchim.rownames)
```

You'll need to create a mapping file with the first column corresponding to the sample names (row names) in `seqtab.nochim`. When you create the mapfile in Excel, sort the first column in ascending order. Keep in mind, the sample names (row names) of the `seqtab.nochim` are the names of samples you submitted to the sequencing facility, if they do not correspond to your actual sample number (say coral ID), then you need to account for that in the mapping file.

Note: make sure variables that are continuous (numerical) are numbers, and catagorical variables (i.e. week) that may have been numerical are catagorical instead. In excel, you can change week "0" to "week-0" which is what I did to the mapfile prior to importing it. In excel, you can make a new column using this function: ="channel-"&F2. Then copy and paste to all samples.

Also, if you want the x-axis of plots to be in order (i.e. week-0, week-2, week-6, week-10), you'll need to edit the mapfile to reflect the order (i.e. week-0, week-02, week-06, week-10).

```{r, eval=FALSE, include=TRUE}
mapfile <- read.csv(file="/Volumes/MaiteBackupHD/MPData/MP_ESFmicroplastic_515_806_metadata_4.csv", as.is=TRUE, header = TRUE)

View(mapfile)
```

The *rownames* in `seqtab.nochim` are already set with your sample names (that were submitted to the sequencing facility and that you downloaded from BaseSpace) and they are in ascending order, but the mapfile data.frame does not have row names and they are not in order. Yet. 

```{r, eval=FALSE, include=TRUE}
rownames(mapfile) <- mapfile$sampleid

mapfile <- mapfile[rownames(seqtab.nochim),]

View(mapfile)
```

Now you can combine the ASV table (load it as an OTU table even though it's not), mapfile (your sample metadata), and taxonomy (tax table) into a phyloseq object. If you were using mothur, you wouldn't need the `taxa_are_rows=FALSE` because of how the OTU table outputs.

```{r, eval=FALSE, include=TRUE}
microplastics <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
                    sample_data(mapfile),
                    tax_table(taxa))
```

Look at your phyloseq object and confirm the # of samples and variables. `rank_names` should output the taxonomic classifications, not "Rank1", "Rank2", etc.

```{r, eval=FALSE, include=TRUE}
microplastics # 151 samples as expected

sample_variables(microplastics) 

rank_names(microplastics) # kingdom through species

sample_data(microplastics)
```

## Filter taxa

It's time to remove mitochondria and chloroplasts while keeping only bacteria. First, define the taxa you don't want. Make sure you are putting the right thing in between the double quotes by checking the taxa names. To do that, view the `taxa.w.species` that you used as your `tax_table` for phyloseq. Use the search function to confirm the capitalization and column of the taxa you want to filter out. Then go through the motions of excluding them from the data set. 

```{r, eval=FALSE, include=TRUE}
View(taxa.w.species)

subset.microplastics <- subset_taxa(microplastics, Kingdom == "Bacteria" & 
                                Family !="Mitochondria" & 
                                Order != "Chloroplast")

```

Confirm that the taxa have been filtered out. Use the search function to make sure the taxa were filtered out. 

```{r, eval=FALSE, include=TRUE}
taxa.print.2 <- as.data.frame(tax_table(subset.microplastics))

View(taxa.print.2)
```

## Filter samples

You can manipulate your phyloseq object to reflect different groups of samples. You can also use it to remove positive and negative controls (if they were included in the mapfile and therefore kept in the phyloseq object) so they don't interfere with rarefaction. Note "|" is the symbol for OR and "&" is the symbol for AND (when using `variable==`.

```{r, eval=FALSE, include=TRUE}
sample_data(subset.microplastics)

mp.con <- subset_samples(subset.microplastics, effluent == "CON")
sample_data(mp.con)

mp.tww <- subset_samples(subset.microplastics, effluent == "TWW")
sample_data(subset.microplastics)

```

First, let's test out the ampvis2 package for the venn diagram/core community function.

# Import into AMPVIS

AMPVIS resource [here](https://madsalbertsen.github.io/ampvis2/articles/ampvis2.html). 

Export your OTU/ASV table and taxonomy table from phyloseq and make a new object out of them. Make sure that the first column of the mapfile is the names of your samples from the OTU/ASV table. You may need to just run the code below if your first column is an identifier. Or you may need to import a new .csv. 

```{r, eval=FALSE, include=TRUE}

# CON samples
otu.mp.con <- as.data.frame(otu_table(mp.con)) # may take a minute, 30175 ASVs for 75 variables
taxa.mp.con <- as.data.frame(tax_table(mp.con)) # 30175 ASVs with 7 variables (taxonomy levels)
map.mp.con <- as.matrix(sample_data(mp.con)) # 5 variables (including sample id) for 75 samples

# For ampvis2 to work, make sure the first column name is as follows.
colnames(map.mp.con)[1] <- "SampleID"
rownames(map.mp.con) <- NULL
map.mp.con <- as.data.frame(map.mp.con)
map.mp.con$SampleID <- as.factor(map.mp.con$SampleID)
map.mp.con$effluent <- as.factor(map.mp.con$effluent)
map.mp.con$type <- as.factor(map.mp.con$type)
map.mp.con$wk <- as.factor(map.mp.con$wk)
map.mp.con$pt <- as.factor(map.mp.con$pt)


# TWW samples

otu.mp.tww <- as.data.frame(otu_table(mp.tww)) # may take a minute, 30175 ASVs for 76 variables
taxa.mp.tww <- as.data.frame(tax_table(mp.tww)) # 30175 ASVs with 7 variables (taxonomy levels)
map.mp.tww <- as.matrix(sample_data(mp.tww)) # 5 variables (including sample id) for 76 samples

# For ampvis2 to work, make sure the first column name is as follows.
colnames(map.mp.tww)[1] <- "SampleID"
rownames(map.mp.tww) <- NULL
map.mp.tww <- as.data.frame(map.mp.tww)
map.mp.tww$SampleID <- as.factor(map.mp.tww$SampleID)
map.mp.tww$effluent <- as.factor(map.mp.tww$effluent)
map.mp.tww$type <- as.factor(map.mp.tww$type)
map.mp.tww$wk <- as.factor(map.mp.tww$wk)
map.mp.tww$pt <- as.factor(map.mp.tww$pt)

```

To import into ampvis2, the row names must be the different ASVs and column names must be the different samples. Transpose the ASV table taken from phyloseq since it's set up as the opposite. Then make sure there is a column with ASV id in the ASV table and taxa table.

```{r, eval=FALSE, include=TRUE}
# CON samples
otu.mp.con <- t(otu.mp.con)
otu.mp.con <- as.data.frame(otu.mp.con) # Convert back to data frame 

# For ampvis2 to work, make sure the first column of the ASV table AND taxonomy table contain a column named ASV
otu.mp.con$ASV <- row.names(otu.mp.con)
taxa.mp.con$ASV <- row.names(taxa.mp.con)


# TWW samples
otu.mp.tww <- t(otu.mp.tww)
otu.mp.tww <- as.data.frame(otu.mp.tww)

otu.mp.tww$ASV <- row.names(otu.mp.tww)
taxa.mp.tww$ASV <- row.names(taxa.mp.tww)

```

Ready to load into ampvis. Note: the metadata variables part (compared to the working phyloseq object) when you view your object.

```{r, eval=FALSE, include=TRUE}
# CON samples
amp.mp.con <- amp_load(otutable = otu.mp.con,
                       metadata = map.mp.con,
                       taxonomy = taxa.mp.con)
amp.mp.con

# TWW samples
amp.mp.tww <- amp_load(otutable = otu.mp.tww,
                       metadata = map.mp.tww,
                       taxonomy = taxa.mp.tww)
amp.mp.tww


# IGNORE THIS PART FOR NOW
# Remove wk0
amp.con <- amp_subset_samples(amp.mp.con,
                              wk %in% c("wk02", "wk06", "wk10"),
                              normalise = TRUE)

amp.tww <- amp_subset_samples(amp.mp.tww,
                              wk %in% c("wk02", "wk06", "wk10"),
                              normalise = TRUE)

# CON - Subset object by polymer types
amp.con.glass <- amp_subset_samples(amp.con,
                                pt %in% c("Glass"))
amp.con.hdpe <- amp_subset_samples(amp.con,
                                pt %in% c("HDPE"))
amp.con.ldpe <- amp_subset_samples(amp.con,
                                pt %in% c("LDPE"))
amp.con.pp <- amp_subset_samples(amp.con,
                                pt %in% c("PP"))
amp.con.ps <- amp_subset_samples(amp.con,
                                pt %in% c("PS"))
amp.con.water <- amp_subset_samples(amp.con,
                                pt %in% c("water"))

# TWW - Subset object by polymer types
amp.tww.glass <- amp_subset_samples(amp.tww,
                                pt %in% c("Glass"))
amp.tww.hdpe <- amp_subset_samples(amp.tww,
                                pt %in% c("HDPE"))
amp.tww.ldpe <- amp_subset_samples(amp.tww,
                                pt %in% c("LDPE"))
amp.tww.pp <- amp_subset_samples(amp.tww,
                                pt %in% c("PP"))
amp.tww.ps <- amp_subset_samples(amp.tww,
                                pt %in% c("PS"))
amp.tww.water <- amp_subset_samples(amp.tww,
                                pt %in% c("water"))


```

## Venn Diagram of Core ASVs

You can calculate the number of "core" ASVs shared by groups given thresholds for how frequent the ASVs should be above a certain abundance and occur across a percent of the samples. The default abundance cutoff is 0.1, frequency cutoff is 80. Running this function also returns the average abundance of the ASVs in a particular group.

NOTE: The core analysis will only work for three variables at once (ie. unknown vs. dz vs. healthy).

```{r, eval=FALSE, include=TRUE}

# CON samples

# CON - Glass
v.con.glass <- amp_venn(amp.con.glass, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.con.glass$plot
write.csv(as.matrix(v.con.glass$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.con.glass.csv")

# CON - HDPE
v.con.hdpe <- amp_venn(amp.con.hdpe, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.con.hdpe$plot
write.csv(as.matrix(v.con.hdpe$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.con.hdpe.csv")

# CON - LDPE
v.con.ldpe <- amp_venn(amp.con.ldpe, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.con.ldpe$plot
write.csv(as.matrix(v.con.ldpe$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.con.ldpe.csv")

# CON - PP
v.con.pp <- amp_venn(amp.con.pp, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.con.pp$plot
write.csv(as.matrix(v.con.pp$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.con.pp.csv")

# CON - PS
v.con.ps <- amp_venn(amp.con.ps, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.con.ps$plot
write.csv(as.matrix(v.con.ps$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.con.ps.csv")

# CON - water
v.con.water <- amp_venn(amp.con.water, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.con.water$plot
write.csv(as.matrix(v.con.water$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.con.water.csv")




# TWW samples

# TWW - Glass
v.tww.glass <- amp_venn(amp.tww.glass, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.tww.glass$plot
write.csv(as.matrix(v.tww.glass$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.tww.glass.csv")

# TWW - HDPE
v.tww.hdpe <- amp_venn(amp.tww.hdpe, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.tww.hdpe$plot
write.csv(as.matrix(v.tww.hdpe$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.tww.hdpe.csv")

# TWW - LDPE
v.tww.ldpe <- amp_venn(amp.tww.ldpe, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.tww.ldpe$plot
write.csv(as.matrix(v.tww.ldpe$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.tww.ldpe.csv")

# TWW - PP
v.tww.pp <- amp_venn(amp.tww.pp, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.tww.pp$plot
write.csv(as.matrix(v.tww.pp$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.tww.pp.csv")

# TWW - PS
v.tww.ps <- amp_venn(amp.tww.ps, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.tww.ps$plot
write.csv(as.matrix(v.tww.ps$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.tww.ps.csv")

# TWW - water
v.tww.water <- amp_venn(amp.tww.water, group_by = "wk", cut_a = 0, cut_f = 80, detailed_output = TRUE, normalise = FALSE)
v.tww.water$plot
write.csv(as.matrix(v.tww.water$Otutable), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn 2/v.tww.water.csv")


```

IGNORE THIS PART FOR NOW: Plot individual taxa with metadata.

```{r, eval=FALSE, include=TRUE}

CON.core <- read.csv(file="/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn/venn_CON_asv_table_CORE_plot.csv", 
                     as.is=TRUE)

TWW.core <- read.csv(file="/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Venn/venn_TWW_asv_table_CORE_plot.csv", 
                     as.is=TRUE)

# Plot CON 
ggplot(CON.core, 
       aes(x = pt, y = average)) + # fill = sample
  theme(panel.spacing = unit(1, "lines"), 
        text = element_text(size = 14), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_bar(stat = "identity") +
  #scale_fill_manual(values = distinctColorPalette(200)) +
  scale_x_discrete(drop = FALSE) +
  scale_y_continuous(expand=c(0,0), limits = c(0,1)) +
  guides(fill = guide_legend(reverse = FALSE, keywidth = 1, keyheight = 1)) +
  ylab("Average Relative Abundance\n") +
  xlab("Polymer Type") +
  ggtitle("CON - Average of Core Taxa by Polymer Type")



# Plot TWW
ggplot(TWW.core, 
       aes(x = pt, y = average)) + # fill = sample
  theme(panel.spacing = unit(1, "lines"), 
        text = element_text(size = 14), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_bar(stat = "identity") +
  #scale_fill_manual(values = distinctColorPalette(200)) +
  scale_x_discrete(drop = FALSE) +
  scale_y_continuous(expand=c(0,0), limits = c(0,1)) +
  guides(fill = guide_legend(reverse = FALSE, keywidth = 1, keyheight = 1)) +
  ylab("Average Relative Abundance\n") +
  xlab("Polymer Type") +
  ggtitle("TWW - Average of Core Taxa by Polymer Type")

```



## Alpha diversity

To complete alpha diversity analyses, we can go back to phyloseq objects and functions. 

Note that these data are not rarefied. We'll use observed (total number of ASVs) and Shannon diversity index (richness AND evenness) to analyze alpha diversity between variable types. Shannon diversity index combines richness (observed) and evenness. This means it considers both the number of species and the inequality between species abundances.

First, make sure you identify the sample variables you'd like to analyze.
```{r, eval=FALSE, include=TRUE}
sample_variables(mp.con)

sample_variables(mp.tww)
```

Alpha diversity PLOTS

Compare between polymer types.

```{r, eval=FALSE, include=TRUE}
# CON
alpha.div <- plot_richness(mp.con, 
              x="pt", 
              measures=c("Observed", "Shannon"), 
              color="pt",
              title="CON - Alpha diversity by MP polymer type") 

alpha.div + 
  geom_boxplot(data=alpha.div$data, aes(x=pt, y=value, color=pt), alpha=0.1) + 
  theme(legend.position="none",
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank())

# TWW
alpha.div <- plot_richness(mp.tww, 
              x="pt", 
              measures=c("Observed", "Shannon"), 
              color="pt",
              title="TWW - Alpha diversity by MP polymer type") 

alpha.div + 
  geom_boxplot(data=alpha.div$data, aes(x=pt, y=value, color=pt), alpha=0.1) + 
  theme(legend.position="none",
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank())

```

Compare between weeks.

```{r, eval=FALSE, include=TRUE}
# CON
alpha.div <- plot_richness(mp.con, 
              x="wk", 
              measures=c("Observed", "Shannon"), 
              color="wk",
              title="CON - Alpha diversity by week") 

alpha.div + 
  geom_boxplot(data=alpha.div$data, aes(x=wk, y=value, color=wk), alpha=0.1) + 
  theme(legend.position="none",
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank())

# TWW
alpha.div <- plot_richness(mp.tww, 
              x="wk", 
              measures=c("Observed", "Shannon"), 
              color="wk",
              title="TWW - Alpha diversity by week") 

alpha.div + 
  geom_boxplot(data=alpha.div$data, aes(x=wk, y=value, color=wk), alpha=0.1) + 
  theme(legend.position="none",
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank())

```


Compare between type (CON/TWW vs. water).

```{r, eval=FALSE, include=TRUE}
# CON
alpha.div <- plot_richness(mp.con, 
              x="type", 
              measures=c("Observed", "Shannon"), 
              color="type",
              title="CON - Alpha diversity compared to water") 

alpha.div + 
  geom_boxplot(data=alpha.div$data, aes(x=type, y=value, color=type), alpha=0.1) + 
  theme(legend.position="none",
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank())

# TWW
alpha.div <- plot_richness(mp.tww, 
              x="type", 
              measures=c("Observed", "Shannon"), 
              color="type",
              title="TWW - Alpha diversity compared to water") 

alpha.div + 
  geom_boxplot(data=alpha.div$data, aes(x=type, y=value, color=type), alpha=0.1) + 
  theme(legend.position="none",
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank())

```


Alpha diversity STATS

First, calculate richness for all samples and variables. Variables will need to be categorical so that we can run statistics. 

Then, you need to test the richness values for normality (using the Shapiro Test). The NULL hypothesis is that the samples came from a Normal distribution. This means that if your p-value <= 0.05, then you would reject that the samples came from a Normal distribution.

If values aren't from a normal distribution, you'll need to use the Kruskal-Wallis, Wilcoxon rank sum test, or general linear models with another distribution. Since the variables are categorical, you use Kruskal-Wallis (non-parametric equivalent of ANOVA). Then you can test pairwise within the variables with Wilcoxon Rank Sum Tests (could use "fdr" instead of "bonf").


```{r, eval=FALSE, include=TRUE}
# This will give you all the estimates of alpha diversity in a data frame.
alpha.con <- estimate_richness(mp.con, split=TRUE)
alpha.tww <- estimate_richness(mp.tww, split=TRUE)

# Add the categorical variable to richness and make them factors to properly run statistics
# CON
alpha.con$pt = sample_data(mp.con)$pt
alpha.con$pt <- as.factor(alpha.con$pt)

alpha.con$wk = sample_data(mp.con)$wk
alpha.con$wk <- as.factor(alpha.con$wk)

alpha.con$type = sample_data(mp.con)$type
alpha.con$type <- as.factor(alpha.con$type)

# TWW
alpha.tww$pt = sample_data(mp.tww)$pt
alpha.tww$pt <- as.factor(alpha.tww$pt)

alpha.tww$wk = sample_data(mp.tww)$wk
alpha.tww$wk <- as.factor(alpha.tww$wk)

alpha.tww$type = sample_data(mp.tww)$type
alpha.tww$type <- as.factor(alpha.tww$type)

# Test for normality (applicable to all variables and the statistics run for each variable)
shapiro.test(alpha.con$Observed) # p-value = 0.06794 meaning a normal distribution
shapiro.test(alpha.con$Shannon) # p-value = 0.0002825 meaning not a normal distribution

# TWW
shapiro.test(alpha.tww$Observed) # p-value = 0.3094 meaning a normal distribution
shapiro.test(alpha.tww$Shannon) # p-value = 0.003487 meaning not a normal distribution

```


Now you can test for significant differences for each variable. And you can use this opportunity to summarize alpha diversity values.

Alpha stats for polymer type.

```{r, eval=FALSE, include=TRUE}
# OBSERVED - Summary Statistics
obs.summ.con <- Summarize(Observed ~ pt, data=alpha.con)
obs.summ.con

obs.summ.tww <- Summarize(Observed ~ pt, data=alpha.tww)
obs.summ.tww

# OBSERVED - Save summary results
write.csv(obs.summ.con, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON observed alpha summary by pt.csv")

write.csv(obs.summ.tww, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW observed alpha summary by pt.csv")

# OBSERVED - Test for significant differences 
con.obs.aov <- summary(aov(Observed ~ pt, data=alpha.con))
con.obs.aov # p-value = 0.000000000855

tww.obs.aov <- summary(aov(Observed ~ pt, data=alpha.tww))
tww.obs.aov # p-value = 4.05e-12
  
# OBSERVED - Pairwise comparison
con.obs.tuk <- TukeyHSD(aov(Observed ~ pt, data = alpha.con), "pt")
con.obs.tuk

write.csv(as.data.frame(con.obs.tuk$pt), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON observed alpha by pt.csv")

tww.obs.tuk <- TukeyHSD(aov(Observed ~ pt, data = alpha.tww), "pt")
tww.obs.tuk

write.csv(as.data.frame(tww.obs.tuk$pt), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW observed alpha by pt.csv")




# SHANNON - Summary Statistics
shan.summ.con <- Summarize(Shannon ~ pt, data=alpha.con)
shan.summ.con

shan.summ.tww <- Summarize(Shannon ~ pt, data=alpha.tww)
shan.summ.tww

# SHANNON - Save summary results
write.csv(shan.summ.con, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON shannon alpha summary by pt.csv")

write.csv(shan.summ.tww, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW shannon alpha summary by pt.csv")

# SHANNON - Test for significant differences 
con.shan.krus <- kruskal.test(Shannon ~ pt, data=alpha.con) 
con.shan.krus # p-value = 0.001138

tww.shan.krus <- kruskal.test(Shannon ~ pt, data=alpha.tww) 
tww.shan.krus # p-value = 0.000008424

# SHANNON - Wilcox test for pairwise comparison
con.shan.wil <- pairwise.wilcox.test(alpha.con$Shannon, alpha.con$pt, p.adjust.method="fdr", paired=FALSE) 
con.shan.wil

tww.shan.wil <- pairwise.wilcox.test(alpha.tww$Shannon, alpha.tww$pt, p.adjust.method="fdr", paired=FALSE) 
tww.shan.wil

# SHANNON - Create data frame to save results
con.shan.pair <- as.data.frame(con.shan.wil$p.value)
con.shan.pair <- cbind(con.shan.pair, new_col = "NA")
con.shan.pair <- rbind(con.shan.pair, c("NA", "NA", "NA", "NA", "NA", con.shan.krus$p.value))
rownames(con.shan.pair)[6] <- "Kruskal-Wallis P-value"
colnames(con.shan.pair)[6] <- "Kruskal-Wallis P-value"

tww.shan.pair <- as.data.frame(tww.shan.wil$p.value)
tww.shan.pair <- cbind(tww.shan.pair, new_col = "NA")
tww.shan.pair <- rbind(tww.shan.pair, c("NA", "NA", "NA", "NA", "NA", tww.shan.krus$p.value))
rownames(tww.shan.pair)[6] <- "Kruskal-Wallis P-value"
colnames(tww.shan.pair)[6] <- "Kruskal-Wallis P-value"

# SHANNON - Save statistics results
write.csv(con.shan.pair, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON shannon alpha stats by pt.csv")

write.csv(tww.shan.pair, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW shannon alpha stats by pt.csv")

```

Alpha stats for week.

```{r, eval=FALSE, include=TRUE}
# OBSERVED - Summary Statistics
obs.summ.con <- Summarize(Observed ~ wk, data=alpha.con)
obs.summ.con

obs.summ.tww <- Summarize(Observed ~ wk, data=alpha.tww)
obs.summ.tww

# OBSERVED - Save summary results
write.csv(obs.summ.con, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON observed alpha summary by wk.csv")

write.csv(obs.summ.tww, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW observed alpha summary by wk.csv")

# OBSERVED - Test for significant differences 
con.obs.aov <- summary(aov(Observed ~ wk, data=alpha.con))
con.obs.aov # p-value = 0.0000104

tww.obs.aov <- summary(aov(Observed ~ wk, data=alpha.tww))
tww.obs.aov # p-value = 0.000665
  
# OBSERVED - Pairwise comparison
con.obs.tuk <- TukeyHSD(aov(Observed ~ wk, data = alpha.con), "wk")
con.obs.tuk

write.csv(as.data.frame(con.obs.tuk$wk), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON observed alpha by wk.csv")

tww.obs.tuk <- TukeyHSD(aov(Observed ~ wk, data = alpha.tww), "wk")
tww.obs.tuk

write.csv(as.data.frame(tww.obs.tuk$wk), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW observed alpha by wk.csv")




# SHANNON - Summary Statistics
shan.summ.con <- Summarize(Shannon ~ wk, data=alpha.con)
shan.summ.con

shan.summ.tww <- Summarize(Shannon ~ wk, data=alpha.tww)
shan.summ.tww

# SHANNON - Save summary results
write.csv(shan.summ.con, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON shannon alpha summary by wk.csv")

write.csv(shan.summ.tww, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW shannon alpha summary by wk.csv")

# SHANNON - Test for significant differences 
con.shan.krus <- kruskal.test(Shannon ~ wk, data=alpha.con) 
con.shan.krus # p-value = 0.0000008799

tww.shan.krus <- kruskal.test(Shannon ~ wk, data=alpha.tww) 
tww.shan.krus # p-value = 0.00009557

# SHANNON - Wilcox test for pairwise comparison
con.shan.wil <- pairwise.wilcox.test(alpha.con$Shannon, alpha.con$wk, p.adjust.method="fdr", paired=FALSE) 
con.shan.wil

tww.shan.wil <- pairwise.wilcox.test(alpha.tww$Shannon, alpha.tww$wk, p.adjust.method="fdr", paired=FALSE) 
tww.shan.wil

# SHANNON - Create data frame to save results
con.shan.pair <- as.data.frame(con.shan.wil$p.value)
con.shan.pair <- cbind(con.shan.pair, new_col = "NA")
con.shan.pair <- rbind(con.shan.pair, c("NA", "NA", "NA", con.shan.krus$p.value))
rownames(con.shan.pair)[4] <- "Kruskal-Wallis P-value"
colnames(con.shan.pair)[4] <- "Kruskal-Wallis P-value"

tww.shan.pair <- as.data.frame(tww.shan.wil$p.value)
tww.shan.pair <- cbind(tww.shan.pair, new_col = "NA")
tww.shan.pair <- rbind(tww.shan.pair, c("NA", "NA", "NA", tww.shan.krus$p.value))
rownames(tww.shan.pair)[4] <- "Kruskal-Wallis P-value"
colnames(tww.shan.pair)[4] <- "Kruskal-Wallis P-value"

# SHANNON - Save statistics results
write.csv(con.shan.pair, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON shannon alpha stats by wk.csv")

write.csv(tww.shan.pair, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW shannon alpha stats by wk.csv")

```


Alpha stats for CON/TWW vs. water.

```{r, eval=FALSE, include=TRUE}
# OBSERVED - Summary Statistics
obs.summ.con <- Summarize(Observed ~ type, data=alpha.con)
obs.summ.con

obs.summ.tww <- Summarize(Observed ~ type, data=alpha.tww)
obs.summ.tww

# OBSERVED - Save summary results
write.csv(obs.summ.con, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON observed alpha summary compared to water.csv")

write.csv(obs.summ.tww, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW observed alpha summary compared to water.csv")

# OBSERVED - Test for significant differences 
con.obs.aov <- summary(aov(Observed ~ type, data=alpha.con))
con.obs.aov # p-value = 1.56e-12

tww.obs.aov <- summary(aov(Observed ~ type, data=alpha.tww))
tww.obs.aov # p-value = 1.93e-12
  
# OBSERVED - Pairwise comparison
con.obs.tuk <- TukeyHSD(aov(Observed ~ type, data = alpha.con), "type")
con.obs.tuk

write.csv(as.data.frame(con.obs.tuk$type), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON observed alpha compared to water.csv")

tww.obs.tuk <- TukeyHSD(aov(Observed ~ type, data = alpha.tww), "type")
tww.obs.tuk

write.csv(as.data.frame(tww.obs.tuk$type), "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW observed alpha compared to water.csv")




# SHANNON - Summary Statistics
shan.summ.con <- Summarize(Shannon ~ type, data=alpha.con)
shan.summ.con

shan.summ.tww <- Summarize(Shannon ~ type, data=alpha.tww)
shan.summ.tww

# SHANNON - Save summary results
write.csv(shan.summ.con, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON shannon alpha summary compared to water.csv")

write.csv(shan.summ.tww, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW shannon alpha summary compared to water.csv")

# SHANNON - Test for significant differences 
con.shan.krus <- kruskal.test(Shannon ~ type, data=alpha.con) 
con.shan.krus # p-value = 0.00002091

tww.shan.krus <- kruskal.test(Shannon ~ type, data=alpha.tww) 
tww.shan.krus # p-value = 0.000003982

# SHANNON - Wilcox test for pairwise comparison
con.shan.wil <- pairwise.wilcox.test(alpha.con$Shannon, alpha.con$type, p.adjust.method="fdr", paired=FALSE) 
con.shan.wil

tww.shan.wil <- pairwise.wilcox.test(alpha.tww$Shannon, alpha.tww$type, p.adjust.method="fdr", paired=FALSE) 
tww.shan.wil

# SHANNON - Create data frame to save results
con.shan.pair <- as.data.frame(con.shan.wil$p.value)
con.shan.pair <- cbind(con.shan.pair, new_col = "NA")
con.shan.pair <- rbind(con.shan.pair, c("NA", con.shan.krus$p.value))
rownames(con.shan.pair)[2] <- "Kruskal-Wallis P-value"
colnames(con.shan.pair)[2] <- "Kruskal-Wallis P-value"

tww.shan.pair <- as.data.frame(tww.shan.wil$p.value)
tww.shan.pair <- cbind(tww.shan.pair, new_col = "NA")
tww.shan.pair <- rbind(tww.shan.pair, c("NA", tww.shan.krus$p.value))
rownames(tww.shan.pair)[2] <- "Kruskal-Wallis P-value"
colnames(tww.shan.pair)[2] <- "Kruskal-Wallis P-value"

# SHANNON - Save statistics results
write.csv(con.shan.pair, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/CON shannon alpha stats compared to water.csv")

write.csv(tww.shan.pair, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Alpha Diversity MiDAS/TWW shannon alpha stats compared to water.csv")

```


## Beta Diversity

You will visualize beta diversity via PCOA and NMDS plots, along with testing whether the centroids or dispersions change.

In some cases, it is appropriate to rarefy samples by sub-sampling the reads to a specific number. When each sample has fairly the same number of reads, the minimum # of reads is used. If you choose to rarefy to a higher # of reads, you could lose samples. Take a look at the total number of reads per sample, keep it in mind, and decide whether you should rarefy or not.

This data set could include conditionally rare taxa (CRT), therefore, keeping all samples without rarefying is a good idea.

First, set up plot colors to facilitate visualization. Then calculate the PCOA and NMDS beta diversity data. You only have to do this once and then plot by variable.

```{r, eval=FALSE, include=TRUE}
plot.colors <- c("steelblue2","purple4","darkorange","firebrick","springgreen4","gold", "darkblue", "darkred", "steelblue", "yellowgreen", "turquoise4", "orange","indianred","darkslategrey", "lightblue","darkgreen","mediumaquamarine","gray48","mediumorchid1", "#5F7FC7","#DA5724", "#508578", "#CBD588","#CD9BCD","#AD6F3B", "#673770","#D14285", "#652926", "#C84248", "#8569D5", "#5E738F","#D1A33D", "#8A7C64", "#599861","dodgerblue","darkmagenta", "forestgreen","steelblue1", "cyan","mediumorchid3", "cadetblue3", "yellow")

# Ordinate a weighted Bray Curtis PCOA (accounting for abundance)
con.ord.pcoa <- ordinate(
  physeq = mp.con, 
  method = "PCoA", 
  distance = "bray",
  weighted = TRUE
)

tww.ord.pcoa <- ordinate(
  physeq = mp.tww, 
  method = "PCoA", 
  distance = "bray",
  weighted = TRUE
)


con.ord.pcoa.2 <- ordinate(
  physeq = subset_samples(mp.con, type != "WATER"), 
  method = "PCoA", 
  distance = "bray",
  weighted = TRUE
)

tww.ord.pcoa.2 <- ordinate(
  physeq = subset_samples(mp.tww, type != "WATER"), 
  method = "PCoA", 
  distance = "bray",
  weighted = TRUE
)

con.ord.pcoa.3 <- ordinate(
  physeq = subset_samples(mp.con, type == "WATER"), 
  method = "PCoA", 
  distance = "bray",
  weighted = TRUE
)

tww.ord.pcoa.3 <- ordinate(
  physeq = subset_samples(mp.tww, type == "WATER"), 
  method = "PCoA", 
  distance = "bray",
  weighted = TRUE
)


# Ordinate a weighted Bray Curtis NMDS
# The stress should be 0.05 to 0.1 for GOOD and 0.1 to 0.2 for Fair 
con.ord.nmds <- ordinate(
  physeq = mp.con, 
  method = "NMDS", 
  distance = "bray"
)
# stress = 0.0961191

con.ord.nmds.2 <- ordinate(
  physeq = subset_samples(mp.con, type != "WATER"), 
  method = "NMDS", 
  distance = "bray"
)
# stress = 0.09497717, no convergence

tww.ord.nmds <- ordinate(
  physeq = mp.tww, 
  method = "NMDS", 
  distance = "bray"
)
# stress = 0.1056729

tww.ord.nmds.2 <- ordinate(
  physeq = subset_samples(mp.tww, type != "WATER"), 
  method = "NMDS", 
  distance = "bray"
)
# stress = 0.09612077

```

To visualize beta diversity, you can use `Principal COORDINATE Analysis` which will calculate Eculicdian distance that can be (1) un-weighted Unifrac distance (using presence and absence of ASVs, taking phylogeny into account) or Bray-Curtis dissimilarity distance (using the influence of ASV abundance - weighted - but no phylogeny). The stat_ellipse() option creates a data "normal twwfidence ellipses" with twwfidence level `level` which is 95% by default to visualize the regression of each sample group. More info here: https://arxiv.org/pdf/1302.4881.pdf.

PCOA: CON by polymer type, week, and compared to water. This same code is re-run for each variable.

```{r, eval=FALSE, include=TRUE}
# PCOA - original
con.pcoa <- plot_ordination(
  physeq = mp.con,
  ordination = con.ord.pcoa,
  color = "wk",
  shape = "pt"
  #default for 'weighted=' is FALSE
  ) + 
  geom_point(aes(color = wk), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("PCOA - CON") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

con.pcoa + stat_ellipse()


# PCOA - no water
con.pcoa <- plot_ordination(
  physeq = subset_samples(mp.con, type != "WATER"),
  ordination = con.ord.pcoa.2,
  color = "wk",
  shape = "pt") + 
  geom_point(aes(color = wk), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("PCOA - CON") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

con.pcoa + stat_ellipse()


# PCOA - just water
con.pcoa <- plot_ordination(
  physeq = subset_samples(mp.con, type == "WATER"),
  ordination = con.ord.pcoa.3,
  color = "wk",
  #shape = "pt"
  ) + 
  geom_point(aes(color = wk), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("PCOA - CON") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

con.pcoa + stat_ellipse()






```


PCOA: TWW by polymer type, week, and compared to water. This same code is re-run for each variable.

```{r, eval=FALSE, include=TRUE}
# PCOA - original
tww.pcoa <- plot_ordination(
  physeq = mp.tww,
  ordination = tww.ord.pcoa,
  color = "wk",
  shape = "pt"
  #default for 'weighted=' is FALSE
  ) + 
  geom_point(aes(color = wk), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("PCOA - TWW") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

tww.pcoa  # + stat_ellipse()

# PCOA - no water
tww.pcoa <- plot_ordination(
  physeq = subset_samples(mp.tww, type != "WATER"),
  ordination = tww.ord.pcoa.2,
  color = "wk",
  shape = "pt") + 
  geom_point(aes(color = wk), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("PCOA - TWW") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

tww.pcoa # + stat_ellipse()

# PCOA - just water 
tww.pcoa <- plot_ordination(
  physeq = subset_samples(mp.tww, type == "WATER"),
  ordination = tww.ord.pcoa.3,
  color = "wk",
  #shape = "wk"
  ) + 
  geom_point(aes(color = wk), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("PCOA - TWW") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

tww.pcoa + stat_ellipse()

sample_data(mp.tww)

```


NMDS: CON by polymer type, week, and compared to water. This same code is re-run for each variable.

```{r, eval=FALSE, include=TRUE}
# Create NMDS 
con.nmds <- plot_ordination(
  physeq = mp.con,
  ordination = con.ord.nmds,
  color = "wk",
  shape = "wk",
  #title = "Title"
) + 
  geom_point(aes(color = wk), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("NMDS - CON wk") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

con.nmds + stat_ellipse()


# Create NMDS 
con.nmds <- plot_ordination(
  physeq = subset_samples(mp.con, type != "WATER"),
  ordination = con.ord.nmds.2,
  color = "pt",
  shape = "pt") + 
  geom_point(aes(color = pt), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("NMDS - CON pt (no water)") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

con.nmds + stat_ellipse()

```


NMDS: TWW by polymer type, week, and compared to water. This same code is re-run for each variable.

```{r, eval=FALSE, include=TRUE}
# Create NMDS 
tww.nmds <- plot_ordination(
  physeq = mp.tww,
  ordination = tww.ord.nmds,
  color = "wk",
  shape = "pt",
  #title = "Title"
) + 
  geom_point(aes(color = wk), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("NMDS - TWW wk") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

tww.nmds + stat_ellipse()


# Create NMDS 
tww.nmds <- plot_ordination(
  physeq = subset_samples(mp.tww, type != "WATER"),
  ordination = tww.ord.nmds.2,
  color = "pt",
  shape = "pt") + 
  geom_point(aes(color = pt), size = 5) + 
  scale_color_manual(values = plot.colors) +
  scale_shape_manual(values = c(16,15,17,18,20,21,22,23,24,25)) +
  ggtitle("NMDS - TWW pt (no water)") +
  theme(plot.title = element_text(size = 18),
        text = element_text(size = 18), 
        axis.title = element_text(size = 15),
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        legend.text = element_text(size = 16))

tww.nmds + stat_ellipse()

```


Statistics for beta-diversity include (1) testing that treatments have different centroids, and (2) testing for changes in dispersion between samples.

```{r, eval=FALSE, include=TRUE}
# (1) Testing the hypothesis that sample groups have different centroids
# Calculate Bray-Curtis distance matrix

# This object represents the phyloseq beta-diversity. 
subset.microplastics.bray.CON <- phyloseq::distance(subset_samples(mp.con, type == "WATER"), method = "bray") 
subset.microplastics.bray.TWW <- phyloseq::distance(subset_samples(mp.con, type == "WATER"), method = "bray") 

# Make a data frame from the sample_data() from phyloseq
subset.microplastics.data.CON <- data.frame(sample_data(subset_samples(mp.con, type == "WATER"))) 
subset.microplastics.data.TWW <- data.frame(sample_data(subset_samples(mp.con, type == "WATER"))) 

```

Now you are ready to perform beta diversity statistics. To test for significance in Bray-Curtis dissimilarity centroids, you perform an Adonis test (aka PERMANOVA). If significant, we can reject the null hypothesis that our treatments have the same centroid or not significantly different. [More info here:](https://www.rdocumentation.org/packages/RVAideMemoire/versions/0.9-69-3/topics/pairwise.perm.manova). You follow the Adonis test with a pairwise PERMANOVA to ensure any significance is true. Sometimes pairwise comparisons show no differences between sample groups.

Centroid by source (all MPs and weeks).

```{r, eval=FALSE, include=TRUE}
# ADONIS - Test for significant difference in centroids
adon.CON <- adonis(subset.microplastics.bray.CON ~ pt, data=subset.microplastics.data.CON, method="bray")
adon.CON # p-value = 0.001
#adon.eff2.tbl <- adon.eff2$aov.tab

adon.TWW <- adonis(subset.microplastics.bray.TWW ~ wk, data=subset.microplastics.data.TWW, method="bray")
adon.TWW # p-value = 0.004


# PERMANOVA - Testing pairwise between sample groups
CON.perm <- pairwise.perm.manova(subset.microplastics.bray.CON, subset.microplastics.data.CON$wk, nperm=999)
CON.perm$p.value

TWW.perm <- pairwise.perm.manova(subset.microplastics.bray.TWW, subset.microplastics.data.TWW$wk, nperm=999)
TWW.perm$p.value

```


--- STOP HERE ---



```{r, eval=FALSE, include=TRUE}
# Create a table of results you can export
centroid.eff2 <- as.data.frame(eff.perm$p.value)
centroid.eff2 <- cbind(centroid.eff2, new_col = "NA")
centroid.eff2 <- rbind(centroid.eff2, c("NA", "NA", "NA", adon.eff2.tbl[1,6]))
rownames(centroid.eff2)[4] <- "Centroid Adonis P-value"
colnames(centroid.eff2)[4] <- "Centroid Adonis P-value"

# Save statistical output
write.csv(centroid.eff2, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Beta Diversity 2/beta centroids by source_all MPs and weeks.csv")
```

Centroid by MPs (all sources and weeks).

```{r, eval=FALSE, include=TRUE}
# ADONIS - Test for significant difference in centroids
adon.pt <- adonis(subset.microplastics.bray ~ pt, data=subset.microplastics.data, method="bray")
adon.pt # p-value = 0.001
adon.pt.tbl <- adon.pt$aov.tab

# PERMANOVA - Testing pairwise between sample groups
pt.perm <- pairwise.perm.manova(subset.microplastics.bray, subset.microplastics.data$pt, nperm=999)
pt.perm$p.value

# Create a table of results you can export
centroid.pt <- as.data.frame(pt.perm$p.value)
centroid.pt <- cbind(centroid.pt, new_col = "NA")
centroid.pt <- rbind(centroid.pt, c("NA", "NA", "NA",
                                    "NA", "NA", adon.pt.tbl[1,6]))
rownames(centroid.pt)[6] <- "Centroid Adonis P-value"
colnames(centroid.pt)[6] <- "Centroid Adonis P-value"

# Save statistical output
write.csv(centroid.pt, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Beta Diversity 2/beta centroids by MPs_all sources and weeks.csv")

```

Centroid by source over time (all MPs).

```{r, eval=FALSE, include=TRUE}
# ADONIS - Test for significant difference in centroids
adon.wk.eff <- adonis(subset.microplastics.bray ~ bywk_eff, data=subset.microplastics.data, method="bray")
adon.wk.eff # p-value = 0.001
adon.wk.eff.tbl <- adon.wk.eff$aov.tab

# PERMANOVA - Testing pairwise between sample groups
wk.eff.perm <- pairwise.perm.manova(subset.microplastics.bray, subset.microplastics.data$bywk_eff, nperm=999)
wk.eff.perm$p.value

# Create a table of results you can export
centroid.wk.eff <- as.data.frame(wk.eff.perm$p.value)
centroid.wk.eff <- cbind(centroid.wk.eff, new_col = "NA")
centroid.wk.eff <- rbind(centroid.wk.eff, c("NA", "NA", "NA", "NA", 
                                            "NA", "NA", "NA", "NA",
                                            "NA", "NA", "NA", "NA", adon.wk.eff.tbl[1,6]))
rownames(centroid.wk.eff)[13] <- "Centroid Adonis P-value"
colnames(centroid.wk.eff)[13] <- "Centroid Adonis P-value"

# Save statistical output
write.csv(centroid.wk.eff, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Beta Diversity 2/beta centroids by source over time_all MPs.csv")
```

Centroid by MPs over time (all sources).

```{r, eval=FALSE, include=TRUE}
# ADONIS - Test for significant difference in centroids
adon.wk.pt <- adonis(subset.microplastics.bray ~ bywk_pt, data=subset.microplastics.data, method="bray")
adon.wk.pt # p-value = 0.001
adon.wk.pt.tbl <- adon.wk.pt$aov.tab

# PERMANOVA - Testing pairwise between sample groups
wk.pt.perm <- pairwise.perm.manova(subset.microplastics.bray, subset.microplastics.data$bywk_pt, nperm=999)
wk.pt.perm$p.value

# Create a table of results you can export
centroid.wk.pt <- as.data.frame(wk.pt.perm$p.value)
centroid.wk.pt <- cbind(centroid.wk.pt, new_col = "NA")
centroid.wk.pt <- rbind(centroid.wk.pt, c("NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA",
                                          "NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA", "NA", adon.wk.pt.tbl[1,6]))
rownames(centroid.wk.pt)[19] <- "Centroid Adonis P-value"
colnames(centroid.wk.pt)[19] <- "Centroid Adonis P-value"

# Save statistical output
write.csv(centroid.wk.pt, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Beta Diversity 2/beta centroids by MPs over time_all sources.csv")


```

To test for homogeneity of dispersion test (aka PERMDISP2), we perform a betadisper() calculation then use permutest() to check for pairwise differences. If significant, we reject the null hypothesis that our treatments have the same dispersions. When this result is NOT significant, we can be more confident that our Adonis result (from the centroid statistics) is a real result, and not due to differences in group dispersions. The first step is to calculate Eigenvalues.

First, you have to make a copy of the data to allow you to make variables factors.

```{r, eval=FALSE, include=TRUE}
subset.microplastics.data.2 <- data.frame(sample_data(subset.microplastics))

```

Compare by effluent. 

```{r, eval=FALSE, include=TRUE}
# Identify variable as factor for statistical analysis
subset.microplastics.data.2$effluent2 <- factor(subset.microplastics.data.2$effluent2)

# Calculate eigenvalues and test for significance
beta.dis <- betadisper(subset.microplastics.bray, subset.microplastics.data.2$effluent2)
perm.eff2 <- permutest(beta.dis, pairwise=TRUE)

# Extract p-value 
perm.eff2.pvalue <- as.data.frame(perm.eff2$tab)

# Create table to save results
stat.perm.eff2 <- as.data.frame(perm.eff2$pairwise$permuted)
stat.perm.eff2 <- cbind(stat.perm.eff2, new_col = "NA")
stat.perm.eff2 <- rbind(stat.perm.eff2, c("NA", perm.eff2.pvalue[1,6]))
rownames(stat.perm.eff2)[7] <- "Permutest P-value"
colnames(stat.perm.eff2)[2] <- "Permutest P-value"

# Save statistical output
write.csv(stat.perm.eff2, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Beta Diversity 2/beta dispersion by source_all MPs and weeks.csv")

# Visualize
boxplot(beta.dis, 
        xlab = "effluent2", 
        ylab = "Distance to centroid", 
        main = "Dispersion by effluent",
        las = 1, 
        par(mar = c(5, 5, 2, 1)),
        ylim = c(0.4, 0.8),
        cex.lab = 1.8, cex.axis = 1.5, cex.main = 1.5)
stripchart(beta.dis$distances ~ subset.microplastics.data.2$effluent2,
           vertical = TRUE, 
           method = "jitter",
           pch = 21, 
           col = "gray", bg = "gray", add = TRUE)

```

Compare by polymer type.

```{r, eval=FALSE, include=TRUE}
# Identify variable as factor for statistical analysis
subset.microplastics.data.2$pt <- factor(subset.microplastics.data.2$pt)

# Calculate eigenvalues and test for significance
beta.dis <- betadisper(subset.microplastics.bray, subset.microplastics.data.2$pt)
perm.pt <- permutest(beta.dis, pairwise=TRUE)

# Extract p-value 
perm.pt.pvalue <- as.data.frame(perm.pt$tab)

# Create table to save results
stat.perm.pt <- as.data.frame(perm.pt$pairwise$permuted)
stat.perm.pt <- cbind(stat.perm.pt, new_col = "NA")
stat.perm.pt <- rbind(stat.perm.pt, c("NA", perm.pt.pvalue[1,6]))
rownames(stat.perm.pt)[16] <- "Permutest P-value"
colnames(stat.perm.pt)[2] <- "Permutest P-value"

# Save statistical output
write.csv(stat.perm.pt, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Beta Diversity 2/beta dispersion by MPs_all sources and weeks.csv")

# Visualize
boxplot(beta.dis, 
        xlab = "polymer_type", 
        ylab = "Distance to centroid", 
        main = "Dispersion by MP",
        las = 1, 
        par(mar = c(5, 5, 2, 1)),
        ylim = c(0.4, 0.8),
        cex.lab = 1.8, cex.axis = 1.5, cex.main = 1.5)
stripchart(beta.dis$distances ~ subset.microplastics.data.2$pt,
           vertical = TRUE, 
           method = "jitter",
           pch = 21, 
           col = "gray", bg = "gray", add = TRUE)
```

Compare by source over time (all MPs).

```{r, eval=FALSE, include=TRUE}
# Identify variable as factor for statistical analysis
subset.microplastics.data.2$bywk_eff <- factor(subset.microplastics.data.2$bywk_eff)

# Calculate eigenvalues and test for significance
beta.dis <- betadisper(subset.microplastics.bray, subset.microplastics.data.2$bywk_eff)
perm.bywk.eff <- permutest(beta.dis, pairwise=TRUE)

# Extract p-value 
perm.bywk.eff.pvalue <- as.data.frame(perm.bywk.eff$tab)

# Create table to save results
stat.perm.bywk.eff <- as.data.frame(perm.bywk.eff$pairwise$permuted)
stat.perm.bywk.eff <- cbind(stat.perm.bywk.eff, new_col = "NA")
stat.perm.bywk.eff <- rbind(stat.perm.bywk.eff, c("NA", perm.bywk.eff.pvalue[1,6]))
rownames(stat.perm.bywk.eff)[79] <- "Permutest P-value"
colnames(stat.perm.bywk.eff)[2] <- "Permutest P-value"

# Save statistical output
write.csv(stat.perm.bywk.eff, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Beta Diversity 2/beta dispersion by source over time_all MPs.csv")

# Visualize
boxplot(beta.dis, 
        xlab = "bywk_eff", 
        ylab = "Distance to centroid", 
        main = "Dispersion by source over time (all MPs)",
        las = 1, 
        par(mar = c(5, 5, 2, 1)),
        ylim = c(0, 0.8),
        cex.lab = 1.8, cex.axis = 0.5, cex.main = 1.5)
stripchart(beta.dis$distances ~ subset.microplastics.data.2$bywk_eff,
           vertical = TRUE, 
           method = "jitter",
           pch = 21, 
           col = "gray", bg = "gray", add = TRUE)
```

Compare by MPs over time (all sources).

```{r, eval=FALSE, include=TRUE}
# Identify variable as factor for statistical analysis
subset.microplastics.data.2$bywk_pt <- factor(subset.microplastics.data.2$bywk_pt)

# Calculate eigenvalues and test for significance
beta.dis <- betadisper(subset.microplastics.bray, subset.microplastics.data.2$bywk_pt)
perm.bywk.pt <- permutest(beta.dis, pairwise=TRUE)

# Extract p-value 
perm.bywk.pt.pvalue <- as.data.frame(perm.bywk.pt$tab)

# Create table to save results
stat.perm.bywk.pt <- as.data.frame(perm.bywk.pt$pairwise$permuted)
stat.perm.bywk.pt <- cbind(stat.perm.bywk.pt, new_col = "NA")
stat.perm.bywk.pt <- rbind(stat.perm.bywk.pt, c("NA", perm.bywk.pt.pvalue[1,6]))
rownames(stat.perm.bywk.pt)[172] <- "Permutest P-value"
colnames(stat.perm.bywk.pt)[2] <- "Permutest P-value"

# Save statistical output
write.csv(stat.perm.bywk.pt, "/Volumes/MaiteBackupHD/MPData/MP_515_806/Figures & Summaries/Beta Diversity 2/beta dispersion by MP over time (all sources).csv")

# Visualize
boxplot(beta.dis, 
        xlab = "bywk_pt", 
        ylab = "Distance to centroid", 
        main = "Dispersion by MP over time (all sources)",
        las = 1, 
        par(mar = c(5, 5, 2, 1)),
        ylim = c(0, 0.8),
        cex.lab = 1.8, cex.axis = 0.5, cex.main = 1.5)
stripchart(beta.dis$distances ~ subset.microplastics.data.2$bywk_pt,
           vertical = TRUE, 
           method = "jitter",
           pch = 21, 
           col = "gray", bg = "gray", add = TRUE)
```












## Bar plots 

At this point, you can also make bar plots of the communities (ASVs ID'ed as bacteria). With R code and ggplot, you can make a bar plot at the Order level of ASVs represented in each treatment. Note: you can and should (depending on the diversity of your samples), filter through low abundance taxa (unless you are looking for a specific taxa's trend in the big picture). If you want to look at a specific taxa's abundance, it would be best to just pull the abundances into a new data frame and plot a bar or line plot.

If the colors in the barplots are weird or not well matched, see this website: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/

Order Composition Bar Plot: polymer type over time

```{r, eval=FALSE, include=TRUE}
# Agglomorate the phyloseq object at a specific taxonomic level (i.e. Order), transform the ASV counts to relative abundance per sample, filter out low abundance taxa then sort the taxa alphabetically. 

ord.subset.microplastics <- subset.microplastics %>%
  tax_glom(taxrank = "Order") %>%
  transform_sample_counts(function(x) {x/sum(x)}) %>%
  psmelt() %>%
  filter(Abundance > 0.05) %>%
  arrange(Order) 

# Plot 
ggplot(ord.subset.microplastics, 
       aes(x = bywk_pt, y = Abundance, fill = Order)) + 
  theme(panel.spacing = unit(1, "lines"), 
        text=element_text(size = 14), 
        panel.border = element_rect(colour = "black", fill = NA, size = 0.5), 
        panel.background = element_blank(), 
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  geom_bar(stat = "identity") +
  scale_fill_manual(values = distinctColorPalette(200)) +
  scale_x_discrete(drop = FALSE) +
  scale_y_continuous(expand=c(0,0), limits = c(0,1)) +
  guides(fill = guide_legend(reverse = FALSE, keywidth = 1, keyheight = 1)) +
  ylab("Relative Abundance (Orders > 5%) \n") +
  ggtitle("Order Composition by MP over time")




# Plot Type 2
# transformation: # of raw ASV divided by total ASV in each coral sample
transf.MP <- transform_sample_counts(subset.microplastics, function(x) {x/sum(x)})
glom <- tax_glom(transf.MP, taxrank = 'Order')
data_glom <- psmelt(glom)
data_glom$Order <- as.character(data_glom$Order)
data_glom$Order[data_glom$Abundance < 0.05] <- "< 5% abund"
# You can view each unique Order (or how many there are to pick the right number of colors to plot) in a list by using: unique(data_glom$Order)
data_glom$Order <- factor(data_glom$Order, levels = sort(unique(data_glom$Order)))

plot.colors2 <- c("gray48", "steelblue1", "darkred", "#599861", "purple4", 
                  "darkorange", "firebrick", "springgreen4", "mediumaquamarine", "cadetblue3",
                  "mediumorchid3", "steelblue", "yellowgreen", "turquoise4", "orange", 
                  "indianred", "lightblue", "darkgreen", "gold", "mediumorchid1", 
                  "#5F7FC7","steelblue2", "dodgerblue","darkmagenta", "cyan",
                  "#DA5724", "darkblue", "#CBD588", "#CD9BCD", "#AD6F3B",
                  # the following values are not used
                  "#673770", "#D14285", "#652926", "#C84248", "#8569D5", 
                  "#5E738F","#D1A33D", "#8A7C64", "darkslategrey", "yellow", 
                  "#508578", "forestgreen")

# Plot
ggplot(data_glom, aes(x = bywk_pt, y = Abundance, fill = Order)) + 
  geom_bar(aes(), stat = "identity", position = "fill", width = 0.98) +
  scale_fill_manual(values = plot.colors2) + 
  # `values` parameter above can be plot.colors or distinctColorPalette(200)
  theme(legend.position = "bottom", 
        axis.title.x = element_blank(), 
        panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank()) + 
  guides(fill=guide_legend(nrow = 5)) +
  scale_y_continuous(expand = c(0,0)) +
  scale_x_discrete(expand = c(0,0)) +
  ggtitle("Order Composition by MP over time") +
  ylab("Relative Abundance (Orders > 5%) \n") +
  theme(plot.title = element_text(hjust = 0.5, size=17),
        axis.text.x = element_text(size = 10, angle = 45, hjust = 1),
        axis.text.y = element_text(size = 10),
        axis.title=element_text(size=15))

```







## Compare specific members of community

You can take the phyloseq ASV/OTU table to pull out specific members of the community and track their abundance.

First, you have to prepare your abundance counts from the phyloseq object, change the variable types, and separate samples by source. You'll also specify which bacteria you want to look at here.

```{r, eval=FALSE, include=TRUE}
# Convert to relative abundance per sample
transf.MP <- transform_sample_counts(subset.microplastics, function(x) {x/sum(x)})
MP.sp <- tax_glom(transf.MP, taxrank = 'Species') 
MP.sp.df <- psmelt(MP.sp) # 54813 obs. of 20 variables

# Change the type of some variables
MP.sp.df$Sample <- factor(MP.sp.df$Sample)
MP.sp.df$polymer_type <- factor(MP.sp.df$polymer_type)
MP.sp.df$effluent2 <- factor(MP.sp.df$effluent2)

# Separate CON and TWW samples
transf.TWW <- MP.sp.df[MP.sp.df$effluent2 == "TWW",] 
transf.TWW.H2O <- MP.sp.df[MP.sp.df$effluent2 == "TWW-H2O",] 
transf.CON <- MP.sp.df[MP.sp.df$effluent2 == "CON",] 
transf.CON.H2O <- MP.sp.df[MP.sp.df$effluent2 == "CON-H2O",] 

# Look for Stenotrophomonas maltophilia
s.mal.TWW <- transf.TWW[transf.TWW$Species == "maltophilia",] 
s.mal.TWW.H2O <- transf.TWW.H2O[transf.TWW.H2O$Species == "maltophilia",] 
s.mal.CON <- transf.CON[transf.CON$Species == "maltophilia",] 
s.mal.CON.H2O <- transf.CON.H2O[transf.CON.H2O$Species == "maltophilia",] 
# CONCLUSION: looking at the environment in R, S. mal was detected.

# Look for Pseudomonas aeruginosa
p.aer.TWW <- transf.TWW[transf.TWW$Species == "aeruginosa",] 
p.aer.TWW <- p.aer.TWW[p.aer.TWW$Genus == "Pseudomonas",]
p.aer.TWW.H2O <- transf.TWW.H2O[transf.TWW.H2O$Species=="aeruginosa",]
p.aer.TWW.H2O <- p.aer.TWW.H2O[p.aer.TWW.H2O$Genus == "Pseudomonas",]
p.aer.CON <- transf.CON[transf.CON$Species=="aeruginosa",] 
p.aer.CON <- p.aer.CON[p.aer.CON$Genus == "Pseudomonas",]
p.aer.CON.H2O <- transf.CON.H2O[transf.CON.H2O$Species=="aeruginosa",] 
p.aer.CON.H2O <- p.aer.CON.H2O[p.aer.CON.H2O$Genus == "Pseudomonas",]
# CONCLUSION: looking at the environment in R, P. aer was not detected.

# Look for Vibrio
vib.TWW <- transf.TWW[transf.TWW$Genus == "Vibrio",] 
vib.TWW.H2O <- transf.TWW.H2O[transf.TWW.H2O$Genus == "Vibrio",] 
vib.CON <- transf.CON[transf.CON$Genus == "Vibrio",] 
vib.CON.H2O <- transf.CON.H2O[transf.CON.H2O$Genus == "Vibrio",] 
# CONCLUSION: looking at the environment in R, Vibrio was not detected.

# Look for Bdellovibrio
bde.TWW <- transf.TWW[transf.TWW$Genus == "Bdellovibrio",] 
bde.TWW.H2O <- transf.TWW.H2O[transf.TWW.H2O$Genus == "Bdellovibrio",] 
bde.CON <- transf.CON[transf.CON$Genus == "Bdellovibrio",] 
bde.CON.H2O <- transf.CON.H2O[transf.CON.H2O$Genus == "Bdellovibrio",] 
# CONCLUSION: looking at the environment in R, Bdellovibrio was detected.
```

Now, summarize data for statistics to detect any abundance above 0.

```{r, eval=FALSE, include=TRUE}
# S. mal
s.mal.TWW.summ = Summarize(Abundance ~ Sample, data = s.mal.TWW)
s.mal.TWW.summ # no S. mal detected
s.mal.TWW.H2O.summ = Summarize(Abundance ~ Sample, data = s.mal.TWW.H2O)
s.mal.TWW.H2O.summ # no S. mal detected
s.mal.CON.summ = Summarize(Abundance ~ Sample, data = s.mal.CON)
s.mal.CON.summ # S. mal detected in ESF21MP55, ESF21MP54
s.mal.CON.H2O.summ = Summarize(Abundance ~ Sample, data = s.mal.CON.H2O)
s.mal.CON.H2O.summ # no S. mal detected
# CONCLUSION: S. mal is only detected in CON samples from CON

# Bdellvibrio
bde.TWW.summ = Summarize(Abundance ~ Sample, data = bde.TWW)
bde.TWW.summ 
bde.TWW.H2O.summ = Summarize(Abundance ~ Sample, data = bde.TWW.H2O)
bde.TWW.H2O.summ 
bde.CON.summ = Summarize(Abundance ~ Sample, data = bde.CON)
bde.CON.summ 
bde.CON.H2O.summ = Summarize(Abundance ~ Sample, data = bde.CON.H2O)
bde.CON.H2O.summ 
# CONCLUSION: Bdellvibrio is detected in multiple samples.
```

Stenotrophomonas maltophilia plots (where determined to be detected)

```{r, eval=FALSE, include=TRUE}
# S. mal
# Boxplot for detection of S. mal (CON only - where detected) over time
ggplot(s.mal.CON, aes(x = wk, y = Abundance)) + 
  geom_boxplot(outlier.color = "white", 
               aes(fill = wk), 
               alpha = 0.3) +
  geom_jitter(aes(color = wk, shape = effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  #scale_fill_manual(values = c("white", "white")) +
  #scale_color_manual(values = c("black", "black")) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Stenotrophomonas maltophilia - CON only",
       x ="Week", 
       y = "Relative Abundance")

# Boxplot for detection of S. mal (CON only - where detected) by polymer type
ggplot(s.mal.CON, aes(x = polymer_type, y = Abundance)) + 
  geom_boxplot(outlier.color = "white", 
               aes(fill = polymer_type), 
               alpha = 0.3) +
  geom_jitter(aes(color = polymer_type, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Stenotrophomonas maltophilia - CON only",
       x ="Polymer Type", 
       y = "Relative Abundance")
# CONCLUSION: S. mal only found in CON glass.

# Bar plot for detection of S. mal (CON only - where detected) - means by polymer type
ggplot(s.mal.CON, aes(x = polymer_type, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = polymer_type, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Stenotrophomonas maltophilia - CON only",
       x ="Polymer Type", 
       y = "Relative Abundance")

# Bar plot for detection of S. mal (CON only - where detected) - means by week
ggplot(s.mal.CON, aes(x = wk, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = wk, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Stenotrophomonas maltophilia - CON only",
       x ="Week", 
       y = "Relative Abundance")


# Bdellvibrio
# Boxplot for detection of Bde in CON over time
ggplot(bde.CON, aes(x = wk, y = Abundance)) + 
  geom_boxplot(outlier.color = "white", 
               aes(fill = wk), 
               alpha = 0.3) +
  geom_jitter(aes(color = wk, shape = effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio",
       x ="Week", 
       y = "Relative Abundance")

# Boxplot for detection of Bde in CON by polymer type
ggplot(bde.CON, aes(x = polymer_type, y = Abundance)) + 
  geom_boxplot(outlier.color = "white", 
               aes(fill = polymer_type), 
               alpha = 0.3) +
  geom_jitter(aes(color = polymer_type, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio",
       x ="Polymer Type", 
       y = "Relative Abundance")
# CONCLUSION: Bdellvibrio is present at all weeks and polymer types.

# Barplot for detection of Bde in CON by polymer type (bars showing sums of abundances)
ggplot(bde.CON, aes(x = polymer_type, y = Abundance)) + 
  geom_col(fill = "lightgray") +
  geom_jitter(aes(color = polymer_type, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio",
       x ="Polymer Type", 
       y = "Relative Abundance")

# Barplot for detection of Bde in CON by polymer type (bars showing averages of abundances)
ggplot(bde.CON, aes(x = polymer_type, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = polymer_type, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio (CON)",
       x ="Polymer Type", 
       y = "Relative Abundance")

# Barplot for detection of Bde in CON by week (bars showing averages of abundances)
ggplot(bde.CON, aes(x = wk, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = wk, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio (CON)",
       x ="Week", 
       y = "Relative Abundance")

# Barplot for detection of Bde in CON-H2O by polymer type (bars showing averages of abundances)
ggplot(bde.CON.H2O, aes(x = polymer_type, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = polymer_type, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio (CON-H2O)",
       x ="Polymer Type", 
       y = "Relative Abundance")

# Barplot for detection of Bde in CON-H2O by week (bars showing averages of abundances)
ggplot(bde.CON.H2O, aes(x = wk, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = wk, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio (CON-H2O)",
       x ="Week", 
       y = "Relative Abundance")

# Barplot for detection of Bde in TWW by polymer type (bars showing averages of abundances)
ggplot(bde.TWW, aes(x = polymer_type, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = polymer_type, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio (TWW)",
       x ="Polymer Type", 
       y = "Relative Abundance")

# Barplot for detection of Bde in TWW by week (bars showing averages of abundances)
ggplot(bde.TWW, aes(x = wk, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = wk, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio (TWW)",
       x ="Week", 
       y = "Relative Abundance")

# Barplot for detection of Bde in TWW-H2O by polymer type (bars showing averages of abundances)
ggplot(bde.TWW.H2O, aes(x = polymer_type, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = polymer_type, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio (TWW-H2O)",
       x ="Polymer Type", 
       y = "Relative Abundance")

# Barplot for detection of Bde in TWW-H2O by week (bars showing averages of abundances)
ggplot(bde.TWW.H2O, aes(x = wk, y = Abundance)) + 
  stat_summary(geom = "col", fun = mean) +
  stat_summary(fun.data = mean_se, geom = "errorbar") +
  geom_jitter(aes(color = wk, shape=effluent2), 
              position = position_jitterdodge(jitter.width = 0.18, dodge.width = 0.8),
              size = 2.7) +
  theme(text=element_text(size = 15), 
        panel.spacing = unit(1, "lines"), 
        panel.border = element_rect(colour = "white", fill = NA, size = 0.5),
        axis.line = element_line(size = 0.5, colour = "black"),
        panel.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  labs(title = "Bdellvibrio (TWW-H2O)",
       x ="Week", 
       y = "Relative Abundance")

```








